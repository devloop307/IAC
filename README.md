# üèóÔ∏è Infraestructura Enterprise On-Premise
## Sistema de Gesti√≥n Centralizado con Alta Disponibilidad

[![Universidad](https://img.shields.io/badge/Universidad-UPAO-blue.svg)](https://www.upao.edu.pe/)
[![Curso](https://img.shields.io/badge/Curso-Infraestructura%20como%20C√≥digo-green.svg)]()
[![Estado](https://img.shields.io/badge/Estado-Producci√≥n-success.svg)]()
[![IaC](https://img.shields.io/badge/IaC-Terraform%20%2B%20Ansible-purple.svg)]()

---

## üìä Executive Summary

**El Problema:** Las operaciones actuales carecen de una infraestructura centralizada que registre ventas e inventario en tiempo real. Esto genera inconsistencias de datos, duplicidades y p√©rdida cr√≠tica de trazabilidad cuando m√∫ltiples usuarios operan simult√°neamente en la red local. Sin integraci√≥n ordenada de perif√©ricos ni monitoreo unificado, detectar fallas y recuperarse a tiempo se convierte en una tarea reactiva y costosa.

**La Soluci√≥n:** Infraestructura empresarial 100% on-premise, completamente automatizada mediante Infrastructure as Code (IaC), que centraliza operaciones, garantiza consistencia transaccional y proporciona observabilidad en tiempo real.

## üéØ Diagrama de la Soluci√≥n


Nuestra soluci√≥n implementa una arquitectura de **tres capas** con segmentaci√≥n VLAN para m√°xima seguridad y rendimiento:

<img width="3740" height="2183" alt="image" src="https://github.com/user-attachments/assets/704543a5-32a2-4aad-b524-40447f9c2d79" />

### üìê Decisiones de Dise√±o

#### ¬øPor qu√© Proxmox VE?
- **Virtualizaci√≥n tipo 1** (bare-metal) con overhead m√≠nimo (<5%)
- **Gesti√≥n centralizada** v√≠a WebUI y API REST completa
- **Clustering nativo** para alta disponibilidad
- **Backup incremental** con Proxmox Backup Server
- **Costo:** $0 (Open Source) vs VMware vSphere (~$6,000/socket)

#### ¬øPor qu√© PostgreSQL sobre MySQL/MongoDB?
- **ACID compliant** con transacciones distribuidas (2PC)
- **JSON nativo** (JSONB) con indexaci√≥n GIN para documentos
- **Window functions** y CTEs recursivos para an√°lisis complejos
- **Replicaci√≥n streaming** con WAL para DR
- **Extensiones:** PostGIS, pg_stat_statements, TimescaleDB

#### ¬øPor qu√© Redis?
- **Sub-milisegundo latency** para cach√© de sesiones
- **Pub/Sub nativo** para eventos en tiempo real
- **Estructuras de datos avanzadas** (Sorted Sets, HyperLogLog)
- **Persistencia configurable** (RDB + AOF)

#### ¬øPor qu√© RabbitMQ?
- **Garant√≠a de entrega** (at-least-once, exactly-once)
- **Dead Letter Queues** para manejo de errores
- **Priorizaci√≥n de mensajes** y TTL configurable
- **Federation y Shovel** para multi-datacenter

---

## üèõÔ∏è Componentes de la Infraestructura

### Capa 1: DMZ (VLAN 10 - 10.10.0.0/24)

#### **Nginx Reverse Proxy** `10.10.0.2:80`
- **Prop√≥sito:** Punto de entrada √∫nico a la infraestructura
- **Funcionalidades:**
  - SSL/TLS termination con certificados Let's Encrypt
  - Load balancing round-robin con health checks
  - Rate limiting (100 req/min por IP)
  - Compresi√≥n gzip/brotli autom√°tica
  - Logs estructurados a ElasticSearch

```nginx
upstream backend {
    least_conn;
    server 10.20.0.2:8080 max_fails=3 fail_timeout=30s;
    server 10.20.0.3:8080 backup;
}
```

#### **Certificados SSL Automatizados**
- Renovaci√≥n autom√°tica v√≠a cron job
- Wildcard certificates para `*.local-app.net`
- HSTS header con preload

### Capa 2: Middleware (VLAN 20 - 10.20.0.0/24)

#### **RabbitMQ Cluster** `10.20.0.2:5672`
- **Prop√≥sito:** Message broker para comunicaci√≥n as√≠ncrona
- **Configuraci√≥n:**
  - 3 nodos en HA (High Availability)
  - Mirrored queues con auto-sync
  - Management plugin en puerto 15672
  - M√©tricas exportadas a Prometheus

**Casos de Uso:**
- Procesamiento as√≠ncrono de √≥rdenes
- Notificaciones push a clientes
- Sincronizaci√≥n de inventario cross-site

### Capa 3: Data Layer (VLAN 30 - 10.30.0.0/24)

#### **PostgreSQL 15+** `10.30.0.3:5432`
- **Configuraci√≥n:**
  - Streaming replication con 1 standby
  - Connection pooling v√≠a PgBouncer (1000 connections)
  - Autovacuum agresivo para OLTP
  - Particionado por fecha en tablas de transacciones

```sql
-- Ejemplo: Tabla particionada por mes
CREATE TABLE ventas (
    id BIGSERIAL,
    fecha TIMESTAMPTZ NOT NULL,
    monto NUMERIC(12,2),
    PRIMARY KEY (id, fecha)
) PARTITION BY RANGE (fecha);
```

#### **Redis Cluster** `10.30.0.2:6379`
- **Configuraci√≥n:**
  - 3 masters + 3 replicas (sharding autom√°tico)
  - Persistencia RDB cada 5min + AOF
  - Maxmemory policy: allkeys-lru
  - Sentinel para failover autom√°tico

**Casos de Uso:**
- Cach√© de consultas frecuentes (TTL: 5min)
- Sesiones de usuario (TTL: 30min)
- Ranking en tiempo real (Sorted Sets)
- Rate limiting distribuido

---

## üõ†Ô∏è Stack Tecnol√≥gico

| Componente | Versi√≥n | Prop√≥sito | Justificaci√≥n |
|------------|---------|-----------|---------------|
| **Proxmox VE** | 8.1+ | Hypervisor Tipo 1 | Virtualizaci√≥n bare-metal, clustering nativo |
| **PostgreSQL** | 15.4+ | Base de datos transaccional | ACID, JSON nativo, extensibilidad |
| **Redis** | 7.2+ | Cach√© + Message Broker | Latencia sub-ms, estructuras avanzadas |
| **RabbitMQ** | 3.12+ | Message Queue | Garant√≠a de entrega, DLQ, federation |
| **Nginx** | 1.24+ | Reverse Proxy + LB | Alto rendimiento, SSL offloading |
| **Harbor** | 2.9+ | Container Registry | Gesti√≥n privada de im√°genes Docker |
| **Grafana** | 10.2+ | Observabilidad | Dashboards + alerting |
| **Prometheus** | 2.48+ | M√©tricas | Time-series DB, service discovery |
| **Terraform** | 1.6+ | Provisioning IaC | Multi-provider, state management |
| **Ansible** | 2.16+ | Configuration Management | Idempotencia, playbooks reusables |

---

## ‚ú® Caracter√≠sticas Clave

### üöÄ Rendimiento
- **Latencia ultra-baja:** < 5ms promedio en LAN (99 percentil: 12ms)
- **Throughput:** 10,000 transacciones/segundo sostenidas
- **Sin dependencia de Internet:** 100% operaci√≥n en red local

### üîí Seguridad
- **Segmentaci√≥n VLAN:** Aislamiento L2 entre capas
- **Firewall distribuido:** nftables con pol√≠ticas por VLAN
- **Secrets management:** HashiCorp Vault integrado
- **Audit logging:** Todos los accesos registrados en syslog

### üìà Escalabilidad
- **Horizontal:** Agregar nodos al cluster sin downtime
- **Vertical:** Hot-add CPU/RAM en VMs
- **Auto-scaling:** Basado en m√©tricas de Prometheus

### üîç Observabilidad
- **Dashboards en tiempo real:** Grafana con refresh cada 5s
- **Alertas proactivas:** PagerDuty integration
- **Log aggregation:** Loki + Promtail
- **Distributed tracing:** Jaeger para microservicios

### üîÑ Automatizaci√≥n Total
- **Provisioning:** Terraform despliega toda la infra en 8 minutos
- **Configuration:** Ansible configura servicios en 3 minutos
- **CI/CD:** GitLab Runner para despliegues continuos
- **Backups:** Snapshots incrementales cada hora

---

## üöÄ Infraestructura como C√≥digo

### Terraform: Provisioning Declarativo

```hcl
# modules/proxmox-vm/main.tf
resource "proxmox_vm_qemu" "postgres_master" {
  name        = "postgres-master"
  target_node = "proxmox-node1"
  
  cores    = 4
  memory   = 8192
  balloon  = 4096
  
  network {
    model  = "virtio"
    bridge = "vmbr30"  # VLAN 30
    tag    = 30
  }
  
  disk {
    type    = "scsi"
    storage = "local-lvm"
    size    = "100G"
    ssd     = 1
  }
  
  lifecycle {
    ignore_changes = [network]
  }
}
```

### Ansible: Gesti√≥n de Configuraci√≥n

```yaml
# playbooks/deploy-postgres.yml
- name: Deploy PostgreSQL Master
  hosts: postgres_master
  become: yes
  roles:
    - postgresql
    - pgbouncer
    - prometheus-exporter
  
  vars:
    postgres_version: 15
    max_connections: 1000
    shared_buffers: "2GB"
    effective_cache_size: "6GB"
    maintenance_work_mem: "512MB"
    checkpoint_completion_target: 0.9
    wal_level: replica
    max_wal_senders: 3
```

### Pipeline de Despliegue

```bash
# deploy.sh - Despliegue completo en 1 comando
#!/bin/bash
set -euo pipefail

echo "üöÄ Iniciando despliegue de infraestructura..."

# 1. Validar Terraform
cd terraform/
terraform validate

# 2. Provisionar VMs
terraform apply -auto-approve

# 3. Esperar boot de VMs (30s)
sleep 30

# 4. Configurar servicios con Ansible
cd ../ansible/
ansible-playbook -i inventory/production site.yml

# 5. Health checks
./scripts/health-check.sh

echo "‚úÖ Infraestructura desplegada exitosamente"
echo "üìä Dashboard: https://local-dashboard.net"
```

---

## üìÅ Estructura del Proyecto

```
infrastructure-upao/
‚îú‚îÄ‚îÄ terraform/
‚îÇ   ‚îú‚îÄ‚îÄ main.tf                    # Configuraci√≥n principal
‚îÇ   ‚îú‚îÄ‚îÄ variables.tf               # Variables de entorno
‚îÇ   ‚îú‚îÄ‚îÄ outputs.tf                 # Outputs (IPs, URLs)
‚îÇ   ‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ proxmox-vm/            # M√≥dulo para VMs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ network/               # VLANs y bridges
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ storage/               # Vol√∫menes y backups
‚îÇ   ‚îî‚îÄ‚îÄ environments/
‚îÇ       ‚îú‚îÄ‚îÄ dev/
‚îÇ       ‚îú‚îÄ‚îÄ staging/
‚îÇ       ‚îî‚îÄ‚îÄ production/
‚îÇ
‚îú‚îÄ‚îÄ ansible/
‚îÇ   ‚îú‚îÄ‚îÄ ansible.cfg
‚îÇ   ‚îú‚îÄ‚îÄ site.yml                   # Playbook principal
‚îÇ   ‚îú‚îÄ‚îÄ inventory/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ production/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hosts.yml
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ group_vars/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ staging/
‚îÇ   ‚îú‚îÄ‚îÄ roles/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ common/                # Configuraci√≥n base
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nginx/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ postgresql/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ redis/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rabbitmq/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ monitoring/
‚îÇ   ‚îî‚îÄ‚îÄ playbooks/
‚îÇ       ‚îú‚îÄ‚îÄ deploy-app.yml
‚îÇ       ‚îú‚îÄ‚îÄ backup.yml
‚îÇ       ‚îî‚îÄ‚îÄ disaster-recovery.yml
‚îÇ
‚îú‚îÄ‚îÄ monitoring/
‚îÇ   ‚îú‚îÄ‚îÄ grafana/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dashboards/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ infrastructure.json
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database.json
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ application.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ datasources/
‚îÇ   ‚îú‚îÄ‚îÄ prometheus/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prometheus.yml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ alerts/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rules/
‚îÇ   ‚îî‚îÄ‚îÄ loki/
‚îÇ       ‚îî‚îÄ‚îÄ loki-config.yml
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ deploy.sh                  # Despliegue automatizado
‚îÇ   ‚îú‚îÄ‚îÄ destroy.sh                 # Destrucci√≥n del entorno
‚îÇ   ‚îú‚îÄ‚îÄ backup.sh                  # Backup manual
‚îÇ   ‚îú‚îÄ‚îÄ restore.sh                 # Restauraci√≥n desde backup
‚îÇ   ‚îî‚îÄ‚îÄ health-check.sh            # Verificaci√≥n de salud
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ architecture.md            # Documentaci√≥n de arquitectura
‚îÇ   ‚îú‚îÄ‚îÄ runbook.md                 # Gu√≠a de operaciones
‚îÇ   ‚îú‚îÄ‚îÄ disaster-recovery.md       # Plan de DR
‚îÇ   ‚îî‚îÄ‚îÄ network-topology.md        # Topolog√≠a de red
‚îÇ
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îú‚îÄ‚îÄ terraform-validate.yml
‚îÇ       ‚îî‚îÄ‚îÄ ansible-lint.yml
‚îÇ
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ LICENSE
```

---

## üéØ Gu√≠a de Instalaci√≥n

### Pre-requisitos

- **Proxmox VE 8.1+** instalado y configurado
- **Terraform CLI** v1.6+
- **Ansible** v2.16+
- **Git** para clonar el repositorio
- **SSH access** al servidor Proxmox

### Paso 1: Clonar el Repositorio

```bash
git clone https://github.com/grupo5-upao/infrastructure-upao.git
cd infrastructure-upao
```

### Paso 2: Configurar Variables de Entorno

```bash
# Copiar template de variables
cp terraform/environments/production/terraform.tfvars.example \
   terraform/environments/production/terraform.tfvars

# Editar con tus credenciales
nano terraform/environments/production/terraform.tfvars
```

```hcl
# terraform.tfvars
proxmox_api_url  = "https://192.168.0.150:8006/api2/json"
proxmox_api_token_id = "terraform@pam!token"
proxmox_api_token_secret = "tu-token-secreto"

vlan10_network = "10.10.0.0/24"
vlan20_network = "10.20.0.0/24"
vlan30_network = "10.30.0.0/24"

postgres_password = "cambiar-en-produccion"
redis_password = "cambiar-en-produccion"
rabbitmq_password = "cambiar-en-produccion"
```

### Paso 3: Inicializar Terraform

```bash
cd terraform/
terraform init

# Validar configuraci√≥n
terraform validate

# Ver plan de ejecuci√≥n
terraform plan -out=tfplan
```

### Paso 4: Provisionar Infraestructura

```bash
# Aplicar cambios (toma ~8 minutos)
terraform apply tfplan

# Guardar outputs
terraform output -json > ../ansible/inventory/terraform-outputs.json
```

### Paso 5: Configurar Servicios con Ansible

```bash
cd ../ansible/

# Verificar conectividad
ansible all -i inventory/production -m ping

# Desplegar toda la configuraci√≥n
ansible-playbook -i inventory/production site.yml

# O desplegar por roles espec√≠ficos
ansible-playbook -i inventory/production playbooks/deploy-postgres.yml
ansible-playbook -i inventory/production playbooks/deploy-nginx.yml
```

### Paso 6: Verificar Despliegue

```bash
# Ejecutar health checks
./scripts/health-check.sh

# Verificar servicios
ansible all -i inventory/production -m shell -a "systemctl status postgresql"
ansible all -i inventory/production -m shell -a "systemctl status redis"
```

### Paso 7: Acceder a los Dashboards

```bash
# Configurar DNS local o editar /etc/hosts
echo "10.10.0.2 local-app.net local-dashboard.net" | sudo tee -a /etc/hosts

# Acceder:
# - Aplicaci√≥n: https://local-app.net
# - Grafana: https://local-dashboard.net
# - Proxmox: https://192.168.0.150:8006
# - Harbor: https://192.168.0.150 (Registry privado)
```

---

## üìä Monitoreo y Observabilidad

### Dashboards de Grafana

#### 1. Infrastructure Overview
![Infrastructure Dashboard](docs/images/dashboard-infra.png)

**M√©tricas clave:**
- CPU/RAM/Disk por VM
- Network throughput por VLAN
- VM uptime y disponibilidad
- Storage IOPS y latencia

#### 2. Database Performance
![Database Dashboard](docs/images/dashboard-db.png)

**M√©tricas clave:**
- Queries/segundo (SELECT, INSERT, UPDATE)
- Connection pool utilization
- Cache hit ratio (>95% target)
- Replication lag (<100ms target)
- Table bloat y vacuum activity

#### 3. Application Metrics
![Application Dashboard](docs/images/dashboard-app.png)

**M√©tricas clave:**
- Request rate y latencia (p50, p95, p99)
- Error rate (<0.1% target)
- Active sessions
- Queue depth (RabbitMQ)

### Sistema de Alertas

```yaml
# prometheus/alerts/critical.yml
groups:
  - name: critical
    interval: 30s
    rules:
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Error rate >5% en {{ $labels.instance }}"
          
      - alert: DatabaseDown
        expr: up{job="postgres"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "PostgreSQL down en {{ $labels.instance }}"
          
      - alert: DiskSpaceLow
        expr: (node_filesystem_avail_bytes / node_filesystem_size_bytes) < 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Disco <10% libre en {{ $labels.instance }}"
```

### Canales de Notificaci√≥n
- **Email:** Alertas cr√≠ticas al equipo de operaciones
- **Slack:** Notificaciones en tiempo real al canal #infrastructure
- **PagerDuty:** Escalamiento autom√°tico 24/7

---

## üë• Equipo del Proyecto

**Universidad:** Universidad Privada Antenor Orrego (UPAO)  
**Curso:** Infraestructura como C√≥digo  
**Grupo:** 5  
**Periodo:** 2024-II

### Integrantes

| Nombre | Rol | Responsabilidades |
|--------|-----|-------------------|
| **ALCANTARA RODRIGUEZ, PIERO** | DevOps Lead | Terraform modules, CI/CD pipeline |
| **BAUTISTA REYES, LOURDES** | Database Engineer | PostgreSQL tuning, replication setup |
| **DAVALOS ALFARO, MARISELLA** | Network Architect | VLAN design, firewall rules |
| **LEYVA VALQUI, GABRIEL** | Monitoring Specialist | Grafana dashboards, alerting |
| **RODRIGUEZ GONZALES, ALEJANDRO** | Systems Engineer | Ansible playbooks, automation |

---

## üó∫Ô∏è Roadmap

### ‚úÖ Fase 1: Foundation (Completado)
- [x] Dise√±o de arquitectura de 3 capas
- [x] Provisioning automatizado con Terraform
- [x] Configuration management con Ansible
- [x] Monitoreo b√°sico con Grafana + Prometheus

### üöß Fase 2: Resilience (En Progreso)
- [ ] Backup automatizado cada 6 horas a storage NFS
- [ ] Disaster Recovery con sitio secundario
- [ ] Chaos Engineering con Gremlin
- [ ] Automated failover testing

### üîÆ Fase 3: Advanced Features (Planeado - Q1 2025)
- [ ] Multi-site replication (Trujillo ‚Üî Lima)
- [ ] Kubernetes para microservicios
- [ ] Service Mesh con Istio
- [ ] Machine Learning para capacity planning

### üåü Fase 4: Cloud Hybrid (Planeado - Q2 2025)
- [ ] Hybrid cloud con AWS VPN
- [ ] Cold storage en S3 Glacier
- [ ] CloudWatch integration
- [ ] Multi-cloud DR strategy

---

## üõ°Ô∏è Seguridad

### Controles Implementados

‚úÖ **Network Security**
- Segmentaci√≥n VLAN con ACLs estrictas
- Firewall stateful (nftables) en cada VM
- IDS/IPS con Suricata
- VPN WireGuard para acceso remoto

‚úÖ **Application Security**
- WAF con ModSecurity en Nginx
- Rate limiting: 100 req/min por IP
- SQL injection prevention (prepared statements)
- XSS protection headers

‚úÖ **Data Security**
- Encryption at rest (LUKS para vol√∫menes)
- Encryption in transit (TLS 1.3 only)
- Secrets management con Vault
- Database audit logging

‚úÖ **Access Control**
- RBAC con roles granulares
- MFA obligatorio para Proxmox
- SSH key-based auth (password disabled)
- Principle of least privilege

### Compliance
- **GDPR:** Anonimizaci√≥n de datos personales
- **ISO 27001:** Controles de seguridad documentados
- **CIS Benchmarks:** Hardening seg√∫n est√°ndares

---

## ‚ùì Problemas Conocidos y Soluciones

### 1. Alta latencia intermitente en Redis

**S√≠ntoma:** Picos de latencia >50ms cada ~10 minutos

**Causa Ra√≠z:** Redis persistence (RDB save) bloqueando el hilo principal

**Soluci√≥n:**
```bash
# Ajustar pol√≠tica de persistencia
redis-cli CONFIG SET save "900 1 300 10"
redis-cli CONFIG SET stop-writes-on-bgsave-error no
```

**Alternativa:** Usar Redis Enterprise con persistencia as√≠ncrona

---

### 2. Postgres connection pool exhausted

**S√≠ntoma:** Error "sorry, too many clients already"

**Causa Ra√≠z:** Application no cierra conexiones correctamente

**Soluci√≥n:**
```ini
# /etc/pgbouncer/pgbouncer.ini
[databases]
app_db = host=10.30.0.3 port=5432 dbname=app pool_size=50

[pgbouncer]
max_client_conn = 1000
default_pool_size = 25
reserve_pool_size = 5
reserve_pool_timeout = 3
```

---

### 3. RabbitMQ memory alarm triggered

**S√≠ntoma:** Producers bloqueados, consumers no procesan mensajes

**Causa Ra√≠z:** Acumulaci√≥n de mensajes sin consumir

**Soluci√≥n:**
```bash
# Aumentar l√≠mite de memoria
rabbitmqctl set_vm_memory_high_watermark 0.6

# Configurar TTL en colas
rabbitmqctl set_policy TTL ".*" '{"message-ttl":3600000}' --apply-to queues
```

**Prevenci√≥n:** Implementar backpressure en producers

---

### 4. Terraform state lock timeout

**S√≠ntoma:** `Error acquiring state lock` al ejecutar `terraform apply`

**Causa Ra√≠z:** Ejecuci√≥n previa interrumpida sin liberar lock

**Soluci√≥n:**
```bash
# Force-unlock (usar con precauci√≥n)
terraform force-unlock <LOCK_ID>

# Alternativa: Migrar a backend remoto
terraform {
  backend "consul" {
    address = "10.20.0.4:8500"
    path    = "terraform/state"
    lock    = true
  }
}
```

---

## üìú Licencia

Este proyecto es software libre bajo **GNU GPLv3**. Ver archivo [LICENSE](LICENSE) para m√°s detalles.

```
Copyright (C) 2024 - Grupo 5 UPAO

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License.
```

---

## üìû Contacto y Soporte

**Universidad:** [www.upao.edu.pe](https://www.upao.edu.pe)  
**Repositorio:** [github.com/grupo5-upao/infrastructure-upao](https://github.com/grupo5-upao/infrastructure-upao)  
**Issues:** [github.com/grupo5-upao/infrastructure-upao/issues](https://github.com/grupo5-upao/infrastructure-upao/issues)  

---

<div align="center">

**‚≠ê Si este proyecto te fue √∫til, considera darle una estrella ‚≠ê**

---

Hecho con ‚ù§Ô∏è por el Grupo 5 | UPAO 2024

</div>
